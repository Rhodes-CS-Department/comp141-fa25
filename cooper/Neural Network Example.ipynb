{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "540539e9",
   "metadata": {},
   "source": [
    "# Simple Neural Network (2-4-1) in Pure Python\n",
    "This notebook implements a small neural network from scratch using **only Python lists and loops**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b6b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faacf5db",
   "metadata": {},
   "source": [
    "## Sigmoid Function\n",
    "\n",
    "A node is \"activated\" when the inputs exceed some threshold. A <b>sigmoid</b> function is often used to implement that threshold. (There are math reasons why a function like this is required, but you don't need to worry about that.) The slope of the function can be adjusted as can the location of the step (referred to as the <b>bias</b>). Both of these parameters can be set and trained, but we're not going to worry about that here either.\n",
    "\n",
    "<div>\n",
    "<img src=\"Sigmoid.png\" width=\"400\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b9ec37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb928c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data (OR function)\n",
    "X = [[0,0], [0,1], [1,0], [1,1]]\n",
    "y = [[0], [1], [1], [1]]\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b703ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data (XOR function)\n",
    "X = [[0,0], [0,1], [1,0], [1,1]]\n",
    "y = [[0], [1], [1], [0]]\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7bc52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "# Network Architecture: 2 input notes, 4 hidden nodes, 1 output node\n",
    "# Learning rate - determines how much the parameters change in response to errors\n",
    "\n",
    "HNODES = 4\n",
    "INODES = 2\n",
    "ONODES = 1\n",
    "lr = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d109fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(time.time())\n",
    "\n",
    "# Initialize weights \n",
    "W1 = []\n",
    "for m in range(INODES):\n",
    "    row = []\n",
    "    for n in range(HNODES):\n",
    "        row.append(random.uniform(-1,1))\n",
    "    W1.append(row)\n",
    "print (W1)\n",
    "\n",
    "W2 = []\n",
    "for m in range(ONODES):\n",
    "    row = []\n",
    "    for n in range(HNODES):\n",
    "        row.append(random.uniform(-1, 1))\n",
    "    W2.append(row)\n",
    "print (W2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3a2d50",
   "metadata": {},
   "source": [
    "## Training loop (pure Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47609feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop 'EPOCH' number of times through the training set.\n",
    "\n",
    "EPOCHS = 100000\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    # Initialize the lists that store the values coming out of the hidden and output layers.\n",
    "\n",
    "    hidden = []\n",
    "    output = []\n",
    "\n",
    "    # -------------------------------\n",
    "    # Forward pass\n",
    "    # -------------------------------\n",
    "\n",
    "    for i in range(len(X)):                 # For each list of inputs in the training set.\n",
    "\n",
    "        # *************************\n",
    "        # Input to hidden\n",
    "        # *************************\n",
    "        \n",
    "        h = []                              # Initialize the output list.\n",
    "        for j in range(HNODES):             # For each hidden node ...\n",
    "            s = 0                           # Initialize the sum\n",
    "            for k in range(INODES):         # Then, for each input node ...\n",
    "                s += X[i][k] * W1[k][j]     # Accumulate the product of the training input (X) and the weight coming from that input node (W1)\n",
    "            h.append(sigmoid(s))            # Apply the sigmoid function to that sum and append it to the list h\n",
    "        hidden.append(h)                    # Append the list h to the list \n",
    "\n",
    "       # *************************\n",
    "       # Hidden to output\n",
    "       # *************************\n",
    " \n",
    "        s = 0                               # Initialize the sum\n",
    "        for j in range(HNODES):             # For each hidden node ...\n",
    "            s += h[j] * W2[0][j]            # Accumulate the product of the hidden node output (h) and the weight coming from that hidden node (W2)\n",
    "        output.append([sigmoid(s)])         # Apply the sigmoid function to that sum\n",
    "    \n",
    "\n",
    "    # -------------------------------\n",
    "    # Backpropagation - This implements a bunch of math to update\n",
    "    #     the weights, but you don't need to worry about the details\n",
    "    #     here except to notice that it's just lists, for loops, and\n",
    "    #     math. \n",
    "    # -------------------------------\n",
    "\n",
    "    d_output = []\n",
    "    for i in range(len(y)):\n",
    "        d = [(y[i][0] - output[i][0]) * sigmoid_deriv(output[i][0])]\n",
    "        d_output.append(d)\n",
    "    \n",
    "    d_hidden = []\n",
    "    for i in range(len(hidden)):\n",
    "        dh = []\n",
    "        for j in range(HNODES):\n",
    "            dh.append(d_output[i][0] * W2[0][j] * sigmoid_deriv(hidden[i][j]))\n",
    "        d_hidden.append(dh)\n",
    "    \n",
    "    # Update W2\n",
    "    for j in range(HNODES):\n",
    "        delta = 0\n",
    "        for i in range(len(X)):\n",
    "            delta += hidden[i][j] * d_output[i][0]\n",
    "        W2[0][j] += lr * delta\n",
    "    \n",
    "    # Update W1\n",
    "    for i in range(INODES):\n",
    "        for j in range(HNODES):\n",
    "            delta = 0\n",
    "            for n in range(len(X)):\n",
    "                delta += X[n][i] * d_hidden[n][j]\n",
    "            W1[i][j] += lr * delta\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020cd0ec",
   "metadata": {},
   "source": [
    "## Test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6f2427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is basically implementing just a forward pass through the network and printing the results.\n",
    "\n",
    "# Initialize the test cases and the list used to store the results.\n",
    "\n",
    "X_test = [[0,0], [0,1], [1,0], [1,1]]\n",
    "output_test = []\n",
    "\n",
    "for i in range(len(X_test)):                # For each test case.\n",
    "    h = []                                  # Initialize the hidden node output list\n",
    "    for j in range(HNODES):                 # For each hidden node ...\n",
    "        s = 0                               # Initialize the sum\n",
    "        for k in range(INODES):             # For each input node ...\n",
    "            s += X_test[i][k] * W1[k][j]    # Accumulate the product of the test input (X_test) and the weight coming from that input node (W1)\n",
    "        h.append(sigmoid(s))                # Apply the sigmoid function to that sum and append it to the list h\n",
    "\n",
    "    s = 0                                   # Initialize the sum\n",
    "    for j in range(HNODES):                 # For each hidden node ...\n",
    "        s += h[j] * W2[0][j]                # Accumulate the product of the hidden node output (h) and the weight coming from that hidden node (W2)\n",
    "    output_test.append([sigmoid(s)])        # Apply the sigmoid function to that sum and append it to the network output\n",
    "\n",
    "print(\"Test Cases:\")\n",
    "print(X_test)\n",
    "print(\"Predictions (rounded):\")\n",
    "print([[round(v[0],3)] for v in output_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697c8b81",
   "metadata": {},
   "source": [
    "# The Impact of Training on Learning the XOR Function\n",
    "\n",
    "|Epochs|Test 1|Test 2|Test 3|Test 4|\n",
    "|------|:----:|:----:|:----:|------|\n",
    "|Target| 0.000| 1.000| 1.000| 0.000|\n",
    "|    10| 0.541| 0.520| 0.563| 0.544|\n",
    "|   100| 0.502| 0.484| 0.522| 0.505|\n",
    "|  1000| 0.504| 0.503| 0.497| 0.496|\n",
    "| 10000| 0.162| 0.858| 0.890| 0.122|\n",
    "|100000| 0.017| 0.984| 0.988| 0.014|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63222a9c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
